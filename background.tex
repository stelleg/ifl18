\section{Background} \label{sec:background}

Programming languages fall roughly into two camps: those with
\emph{strict} and those with \emph{non-strict} semantics. A strict language is
one in which arguments at a call-site are always evaluated, while a non-strict
language only evaluates arguments when they are needed. One can further break
non-strict into two categories: call-by-name and call-by-need. Call-by-name is
essentially evaluation by substitution: an argument term or closure is
substituted for every instance of a corresponding variable. This has the
downside that it can result in exponential slowdown due to repeated work: every
variable dereference must re-evaluate the corresponding argument. Call-by-need
is an evaluation strategy devised to address this shortcoming. By sharing the
result of argument evaluation between instances of a variable, one avoids
duplicated work.  Unsurprisingly, call-by-need is the default semantics
implemented by compilers for non-strict languages like Haskell \cite{jonesstg}. 

Also perhaps unsurprisingly, call-by-need implementations tend to be more
complicated than their strict counterparts. For example, even attempts at simple
call-by-need abstract machines such as the Three Instruction Machine \cite{TIM}
require lambda lifting and shared indirections, both of which make formal
reasoning more difficult. Our \ce machine avoids these complications
by using shared environments to share evaluation results between instances of a
variable. We showed in previous work that in addition to being simpler to
implement and reason about, performance of this approach may be able compete
with the state of the art \cite{cem}. 

With recent improvements in higher order logics, machine verification of
algorithms has become a valuable tool in software development. Instead of
relying heavily on tests to check the correctness of programs, verification can
prove that algorithms implement their specification for \emph{all} inputs.
Implementing both the specification and the proof in a machine-checked logic
removes the vast majority of bugs found in hand-written proofs, ensuring far
higher confidence in correctness than other standard methods. Other approaches,
such as fuzz testing, have confirmed that verified programs remove effectively
all bugs \cite{yangfuzz}.

This approach applies particularly well to compilers. Often, the specification
for a compiler is complete: source level semantics for some languages are
exceedingly straightforward to specify, and target architectures have lengthy
specifications that are amenable to mechanization. In addition, writing tests
for compilers that cover all cases is even more hopeless than most domains, due
to the size and complexity of the domain and codomain. The amortized return on
investment is also high: all reasoning about programs compiled with a verified
compiler is provably preserved. 

Due to the complexities discussed above involved in implementing lazy languages,
existing work has focused on compiling strict languages
\cite{chlipala2007certified, leroy2012compcert, cakeml14}. Here we use the
simple \ce machine as a base for a verified compiler of a lazy language, using
the Coq proof assistant. 

As with many areas of research, the devil is in the details. What exactly does
it mean to claim a compiler is verified?  Essentially, a verified compiler of a
functional language is one that preserves computation of values. That is, we
have an implication: \emph{if the source semantics computes a
value, then the compiled code computes an equivalent value}
\cite{chlipala2007certified}. The important thing to note is that the
implication is only in one direction. If the source semantics never terminates,
this class of correctness theorem says nothing about the behavior of the
compiled code. This has consequences for Turing-complete source languages. If we
are unsure if a source program terminates, and wish to run it to check
experimentally if it does, if we run the compiled code and it returns a value,
we cannot be certain that it corresponds to a value computed in the source
semantics. 

While in theory one could solve this by proving the implication the other
direction, that is, \emph{if the compiled code computes a value then the source
semantics computes an equivalent value}, in practice this is prohibitively
difficult. Effectively, the induction rules for the abstract machine make
constructing such a proof monumentally tricky. 

One approach for getting around this issue is to try and capture the divergent
behavior by defining a diverging semantics explicitly \cite{functionalbigstep}.
Then we can safely say that \emph{if the source semantics diverges according to
our diverging semantics, then the compiled code also diverges}. 

For this paper, we choose to take the approach of \cite{chlipala2007certified}
and define verification as the first implication above, focusing on the case in
which the source semantics evaluates to a value. This is still a strong
result: any source program that has meaning compiles to an executable with
equivalent meaning. In addition, if we ever choose to extend the language with a
type system that ensures termination, or some notion of progress, then we can
use this proof in combination with our verification proof to prove the other
direction. 
